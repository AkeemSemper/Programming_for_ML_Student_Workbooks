{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "import random\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "Doing a simple linear regression is cool, but for this to be really useful we want to be able to use a bunch of factors to make a prediction. The process is largely the same, we just have more variables on the input (X) side. It is also difficult/impossible to visualize, since we'd need to see in 3+ dimensions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First, we'll get some data, and look at what the target and the features are going to be. This is a really simplifed example of some EDA that we normally do in advance of machine learning. The first step to any predictive model building is to look at our data and see if anything should be done prior to modelling. This is pretty vague, but we are basically looking to see if there's anything in the data that might be suboptimal for the predictions we want to create. Some common things to look for are:\n",
    "<ul>\n",
    "<li> Errors - anything that is just \"wrong\", words in numeric columns, mislabeled values, etc...\n",
    "<li> Outliers - values too large or too small. \n",
    "<li> Weird Distributions - this is very open-ended, but think of something like a numerical feature where the variance is exceeding low. If every row of the data has the same value, it probably won't help in making predictions, which is inheirently seeking to discriminate. For example, some schools in Japan used to force students to dye their hair black if it wasn't already; if hair color was a feature in your dataset, it wouldn't really add any value if 99.9% of rows of data is \"black\". \n",
    "<li> Missing Values - how should we deal with it if some values are blank?\n",
    "</ul>\n",
    "\n",
    "This isn't a conclusive list, and as we explore we may find other things to change or remove. The amount of work we may need to do can also vary widely - if the data is comming from something like a PoS system it will probably be pretty clean and not require many changes; if the data is coming from scraping social media sites from that people fill out, the data may well require massive amounts of cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "#I'll drop density to make it more realistic.\n",
    "df = pd.read_csv(\"data/bodyfat.csv\")\n",
    "df = df.drop(columns={\"Density\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. We'll predict the BodyFat (which is hard to accurately measure) by using the other stuff, which is easier to measure. \n",
    "\n",
    "Before we start, we want to do some exploration of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Exploring the Data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, it looks like there aren't a lot of issues. One thing I notice is that there's a BodyFat reading of 0 for the min. That isn't possible, so we'll need to get rid of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get variable info. \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our variables are correctly identified as numbers. Often if there's some erroneous data in a column it might get miscategorized as an object. Since we'll be dealing with non-numeric data in ML, starting next workbook, we probably want to double check.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual correlation and distributions\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears there are some outliers that probably aren't helping - e.g. Height. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there nulls?\n",
    "df.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some outliers\n",
    "# These were judgement cals. \n",
    "# E.g. it is theoretically possible for someone to have fat < 5%, but that's really only people like\n",
    "# bodybuilders at a competition time. Those types of extreme outliers probably wont' help our model. \n",
    "df = df[df[\"BodyFat\"] > 5]\n",
    "df = df[df[\"Height\"] > 40]\n",
    "df = df[df[\"Weight\"] < 300]\n",
    "df = df[df[\"Ankle\"] < 30]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine Distributions Post-Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check with cleaned data. \n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good. We may want to examine some of the outliers a bit more closely, but overall the data looks pretty usable. Distributions are all pretty normal. Large outliers are gone. Nothing really jumps out as being a problem.\n",
    "\n",
    "There does appear to be a large amount of correlation between variables. We'll worry about that later on, for now, looks good. Time to start regressing...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the data\n",
    "\n",
    "The basic idea here is the same as one variable regression, only instead of one X, we have a bunch. We can't do this with simple functions, at least not easily, so we'll use a package. \n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "One thing that can cause an issue for us is with the evaluation of the model's accuracy (more in other models than in simple linear regression) is derived from the data we used to create the model. In reality, we want to have an idea of how accurately our model will make predictions on <i>new</i> data that they've never seen before. To accomplish this, we can get a better idea of the quality of our model by splitting the data into two halves - a training part and a testing part. \n",
    "\n",
    "The train_test_split function does this for us in an easy way. It will take in the X and y parts of our original dataset and return both a training and testing array of data for both X and y. So we will train our model with the training data, usually about 70%-80% of the data, then after that model is created we'll test the accuracy on the other ~25% of the data that we held aside for the test set. The results of our model's performance on this \"new\" data will be more representative of the performance we could expect in normal usage, as this data is new as far as the model is concerned. \n",
    "\n",
    "This approach prevents data leakage, meaning that we aren't testing a model on it predicting things it has already seen. We want to remove any possibility of the model \"memorizing\" the data we gave it to train, and giving better performance than we can expect on real new data. Note that this is much less obvious on linear regression than it is with other model types that can be more complex. The difference on linear regression is minimal, the difference with a tree-based model or a neural network model will be much more dramatic. \n",
    "\n",
    "### SciKitLearn Data Setup\n",
    "\n",
    "We can now prepare the data, and put it into its final shape. We need to generate two arrays, one for X and one for y:\n",
    "<ul>\n",
    "<li> X - the features. Should be # of records tall x # of features wide. </li>\n",
    "<li> y - the target. Should be # of records tall x 1 column wide. </li>\n",
    "</ul>\n",
    "\n",
    "This is also the last time that we'll manipulate the data manually - once we create the X/y datasets, then feed those into the train-test split function, we will input that data directly to the ML algorithm as-is. So any data preparation stuff that we want to do ahead of time should be done beforehand. \n",
    "\n",
    "This part of the data setup will also be largely identical in all of our examples - generate X/y, do the train-test split, feed into ML algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y data is simple, do that first\n",
    "y = np.array(df[\"BodyFat\"]).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is identical to the single variable stuff. For X, we have a width to our array. We can take a look again to see what we expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a new df with only the features we'll use\n",
    "df_ = df.drop(columns={\"BodyFat\"})\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make that df into an array. \n",
    "x = np.array(df_)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapes\n",
    "print(\"X shape\", x.shape)\n",
    "print(\"Y shape\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKL Regress...\n",
    "\n",
    "Shape for this one looks good! We have 240 rows, which matches the Y. We have 13 columns, which is what we get if we were to count up the columns by hand. Success!!\n",
    "\n",
    "Time for regression stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate model \n",
    "model = LinearRegression().fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Results\n",
    "\n",
    "We can check how our model performs on the testing data - the 30% of data that we held aside from the model while it was being trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some info on our new regression model\n",
    "r_sq = model.score(xTest, yTest)\n",
    "print('R-squared:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View \"Slopes\" and Intercept\n",
    "\n",
    "In single regression our model was made up of the slope, and intercept values - we can plug any X in and calculate our prediction. In multiple regression, the results are the same, there's just more of them. We still have one intercept, but now each X has it's own m. The entire equation ends up just having a bunch of \"X\" terms:\n",
    "\n",
    "$ y = m1 * x1 + m2 * x2 + m3 * x3 ... + b $\n",
    "\n",
    "We can extract the values just as we did before, we could also make a manual model to generate predictions - our equation would just be longer than the y = mx + b ones that we created earlier. \n",
    "\n",
    "#### Visualizing Multiple Regression\n",
    "\n",
    "The idea of how multiple regression works is the same, it is just harder to visualize. If we try to look at an example with 2 features (so 3 total values, with the target) and picture it a little. In a single regression we make a prediction by doing a \"lookup\" in one dimension - the X value is our input, we slide along the X axis until we find that value, then look up to the regression line to see what the target is at that point. In a 2 feature regression, the same idea applies, only we do the lookup with two X values on a plane and the prediction is where they intersect on the 3rd axis:\n",
    "\n",
    "![3D Regression](images/3d_regression.png \"3D Regression\")\n",
    "\n",
    "Now our model, or our line of best fit, is a plane - a two dimensional line. Each feature we add ups that number of dimensions by 1, and it is always 1 less than the total number of dimensions. In a single regression teh value on one axis predicts the other axis, in a multiple regression the value on [# of features] axis, where they interesect, predicts the value on the other axis. Same, same. \n",
    "\n",
    "Going up in features, the same idea always applies, we just don't have a timecube to draw it. If we had 13 features like we do above, we'd have a scatter plot in 14 dimensional space, and we'd look to the intercept of the 13 features, which would give us our prediction on the 14th axis - the one belonging to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our coefficent/slope is now an array of values - one per X. \n",
    "#Visualizing the regression would be a 14D space, where these are the slopes in each dimension. \n",
    "print('Intercept:', model.intercept_[0])\n",
    "print('Coefs:', model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Metrics\n",
    "\n",
    "Get the residuals to calculate RMSE. \n",
    "\n",
    "The residual stuff is the same no matter how many Xs we have on the input side, since all we are checking is the Y values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get RMSE\n",
    "tmp = model.predict(xTest)\n",
    "mean_squared_error(tmp, yTest, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residuals\n",
    "\n",
    "We don't generally need to view the residuals if we aren't doing things by hand, the library functions allow us to just look at the summary statistics like RMSE. We can view them for fun, just to confirm they exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Residuals and picture them in a DF for easy reading. \n",
    "tmp1 = pd.DataFrame(yTest, columns={\"Y values\"})\n",
    "tmp2 = pd.DataFrame(tmp, columns={\"Predictions\"})\n",
    "tmp3 = pd.DataFrame((yTest-tmp), columns={\"Residual\"})\n",
    "resFrame = pd.concat([tmp1,tmp2,tmp3], axis=1)\n",
    "resFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Predict the Density value with a sklearn linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Density</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Neck</th>\n",
       "      <th>Chest</th>\n",
       "      <th>Abdomen</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Thigh</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>Biceps</th>\n",
       "      <th>Forearm</th>\n",
       "      <th>Wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0708</td>\n",
       "      <td>23</td>\n",
       "      <td>154.25</td>\n",
       "      <td>67.75</td>\n",
       "      <td>36.2</td>\n",
       "      <td>93.1</td>\n",
       "      <td>85.2</td>\n",
       "      <td>94.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>21.9</td>\n",
       "      <td>32.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0853</td>\n",
       "      <td>22</td>\n",
       "      <td>173.25</td>\n",
       "      <td>72.25</td>\n",
       "      <td>38.5</td>\n",
       "      <td>93.6</td>\n",
       "      <td>83.0</td>\n",
       "      <td>98.7</td>\n",
       "      <td>58.7</td>\n",
       "      <td>37.3</td>\n",
       "      <td>23.4</td>\n",
       "      <td>30.5</td>\n",
       "      <td>28.9</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0414</td>\n",
       "      <td>22</td>\n",
       "      <td>154.00</td>\n",
       "      <td>66.25</td>\n",
       "      <td>34.0</td>\n",
       "      <td>95.8</td>\n",
       "      <td>87.9</td>\n",
       "      <td>99.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>38.9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.8</td>\n",
       "      <td>25.2</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0751</td>\n",
       "      <td>26</td>\n",
       "      <td>184.75</td>\n",
       "      <td>72.25</td>\n",
       "      <td>37.4</td>\n",
       "      <td>101.8</td>\n",
       "      <td>86.4</td>\n",
       "      <td>101.2</td>\n",
       "      <td>60.1</td>\n",
       "      <td>37.3</td>\n",
       "      <td>22.8</td>\n",
       "      <td>32.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0340</td>\n",
       "      <td>24</td>\n",
       "      <td>184.25</td>\n",
       "      <td>71.25</td>\n",
       "      <td>34.4</td>\n",
       "      <td>97.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.9</td>\n",
       "      <td>63.2</td>\n",
       "      <td>42.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Density  Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  Knee  \\\n",
       "0   1.0708   23  154.25   67.75  36.2   93.1     85.2   94.5   59.0  37.3   \n",
       "1   1.0853   22  173.25   72.25  38.5   93.6     83.0   98.7   58.7  37.3   \n",
       "2   1.0414   22  154.00   66.25  34.0   95.8     87.9   99.2   59.6  38.9   \n",
       "3   1.0751   26  184.75   72.25  37.4  101.8     86.4  101.2   60.1  37.3   \n",
       "4   1.0340   24  184.25   71.25  34.4   97.3    100.0  101.9   63.2  42.2   \n",
       "\n",
       "   Ankle  Biceps  Forearm  Wrist  \n",
       "0   21.9    32.0     27.4   17.1  \n",
       "1   23.4    30.5     28.9   18.2  \n",
       "2   24.0    28.8     25.2   16.6  \n",
       "3   22.8    32.4     29.4   18.2  \n",
       "4   24.0    32.2     27.7   17.7  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and remove bodyfat\n",
    "dfE = pd.read_csv(\"data/bodyfat.csv\")\n",
    "dfE.drop(columns={\"BodyFat\"}, inplace=True)\n",
    "dfE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((252, 13), (252, 1))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup Data\n",
    "yE = np.array(dfE[\"Density\"]).reshape(-1,1)\n",
    "xE = np.array(dfE.drop(columns={\"Density\"}))\n",
    "xE.shape, yE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and Train\n",
    "xTrainE, xTestE, yTrainE, yTestE = train_test_split(xE,yE,test_size=.3)\n",
    "#Generate model \n",
    "modelE = LinearRegression().fit(xTrainE,yTrainE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.011128385792870505\n",
      "R-squared: 0.6505217281635629\n"
     ]
    }
   ],
   "source": [
    "#Score \n",
    "#Get RMSE and R2\n",
    "tmpE = modelE.predict(xTestE)\n",
    "rmseE = mean_squared_error(tmpE, yTestE, squared=False)\n",
    "r_sqE = modelE.score(xTestE, yTestE)\n",
    "\n",
    "print(\"RMSE:\", rmseE)\n",
    "print('R-squared:', r_sqE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
